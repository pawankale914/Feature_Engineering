{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dbcbe28-d0b0-44c2-bcb4-ad06fd349be9",
   "metadata": {},
   "source": [
    "Q1.What is a parameter?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71b885e5-e581-47ec-ac35-e441955c80f5",
   "metadata": {},
   "source": [
    "A parameter is a variable or value that is used to define or influence the behavior of a system, function, model, or process. Parameters are typically set or specified ahead of time and can be adjusted to modify the outcome or performance of the system.\n",
    "\n",
    "In Mathemtics,\n",
    "A parameter is a variable that helps define a family of functions or equations. For example, in the equation of a line \n",
    "y=mx+b, the parameters are \n",
    "m (slope) and b (y-intercept). Changing these parameters will change the specific line that the equation represents.\n",
    "\n",
    "In Coding,\n",
    "A parameter refers to a value or variable that is passed into a function or method. Parameters are used to provide input to the function, so it can perform its task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ed0a907-9bbb-4790-9032-10a51b000e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(name):\n",
    "    print(f\"Hello, {name}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed98f0-37f7-4acc-8967-2cb98a4d31d5",
   "metadata": {},
   "source": [
    "Q2.What is correlation?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e546c4e-6819-4149-be9c-9ba282c4a46a",
   "metadata": {},
   "source": [
    "Correlation is a statistical measure that describes the strength and direction of a relationship between two or more variables. In simple terms, it shows how one variable changes when another variable changes.\n",
    "\n",
    "Example of Correlation:\n",
    "1.Positive Correlation: The number of hours worked and salary are often positively correlated. As the number of hours worked increases, salary tends to increase.\n",
    "2.Negative Correlation: The number of hours spent watching TV and physical fitness may have a negative correlation. As TV watching increases, physical activity tends to decrease.\n",
    "\n",
    "Types of Correlation:\n",
    "1.Pearson’s Correlation: Measures the linear relationship between two continuous variables.\n",
    "2.Spearman’s Rank Correlation: A non-parametric test that measures the monotonic relationship between two variables (whether or not the relationship is linear).\n",
    "3.Kendall’s Tau: Another non-parametric measure of correlation, similar to Spearman's rank, but generally used for smaller data sets.\n",
    "\n",
    "Visualizing Correlation:\n",
    "Scatter Plot: A scatter plot can be used to visually inspect the relationship between two variables. If the points tend to form a straight line, this suggests a strong linear correlation (positive or negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ffa1d3-2a69-43d8-be06-5e34adf049f5",
   "metadata": {},
   "source": [
    "Q3.Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6dd3e275-3254-4875-84ba-fee01c4dcb16",
   "metadata": {},
   "source": [
    "Machine Learning (ML) is a subfield of artificial intelligence (AI) that focuses on the development of algorithms and models that allow computers to learn from and make decisions based on data, without being explicitly programmed. In machine learning, systems improve their performance on a specific task over time by identifying patterns in data and using these patterns to make predictions or decisions.\n",
    "\n",
    "The core idea behind ML is that instead of programming every decision rule, the system learns from experience, i.e., the data, to improve its ability to make decisions or predictions.\n",
    "\n",
    "Main Components in Machine Learning:\n",
    "1.Data:\n",
    "Data is the foundation of any machine learning model. It consists of the examples (e.g., images, text, numbers) from which the model will learn. The data is usually split into two main subsets:\n",
    "Training Data: This is the data used to teach the model by adjusting the model's parameters.\n",
    "Test Data: This data is used to evaluate the model's performance after training.\n",
    "The quality, quantity, and representativeness of the data are crucial factors in the success of the machine learning model.\n",
    "\n",
    "2.Algorithms:\n",
    "Algorithms are the procedures or methods that enable the system to learn from the data. In machine learning, an algorithm takes in the input data and creates a model that can make predictions or decisions. The choice of algorithm depends on the task and type of data.\n",
    "Examples of popular machine learning algorithms include:\n",
    "Linear Regression (for regression tasks)\n",
    "Decision Trees (for classification and regression tasks)\n",
    "Support Vector Machines (SVMs) (for classification tasks)\n",
    "K-Nearest Neighbors (KNN) (for classification and regression)\n",
    "Neural Networks (for deep learning tasks)\n",
    "Random Forests (an ensemble method for classification and regression)\n",
    "\n",
    "3.Model:\n",
    "A model is the output of a machine learning algorithm after it has been trained on data. It is essentially a mathematical representation of the patterns the algorithm has learned from the training data. A trained model can then be used to make predictions or decisions based on new data (test data or future data).\n",
    "For example, in a spam email detection system, the trained model would learn patterns that differentiate spam emails from non-spam (ham) emails.\n",
    "Features (Input Variables):\n",
    "\n",
    "Features are the individual measurable properties or characteristics of the data that are used as input to the machine learning model. These features are also known as independent variables or predictors.\n",
    "In a house price prediction model, the features might include square footage, number of bedrooms, location, etc.\n",
    "Selecting the right features (called feature selection) is important for the success of the model.\n",
    "Labels (Output Variables):\n",
    "\n",
    "Labels are the target variables or outcomes the model is trying to predict or classify. These are also known as dependent variables.\n",
    "In supervised learning, labels are provided as part of the training data. For example, in a supervised classification task like email spam detection, the label would be the category (\"spam\" or \"not spam\") for each email.\n",
    "Loss Function (Objective Function):\n",
    "\n",
    "The loss function measures how well or poorly the model’s predictions align with the actual values (the labels). The goal of machine learning is typically to minimize the loss function, thereby improving the model's performance.\n",
    "Examples of loss functions include:\n",
    "Mean Squared Error (MSE) for regression tasks.\n",
    "Cross-Entropy Loss for classification tasks.\n",
    "\n",
    "4.Optimization:\n",
    "Optimization refers to the process of adjusting the parameters of the model to minimize the loss function. It is the process through which the model \"learns\" from the data.\n",
    "In many cases, gradient descent is used to minimize the loss function by iteratively updating the parameters in the direction that reduces the error.\n",
    "\n",
    "5.Evaluation Metrics:\n",
    "Evaluation metrics are used to assess the performance of the model. Depending on the type of machine learning task (classification, regression, etc.), different metrics are used:\n",
    "For classification: Accuracy, Precision, Recall, F1-Score, ROC-AUC, etc.\n",
    "For regression: Mean Absolute Error (MAE), Mean Squared Error (MSE), R² (R-squared), etc.\n",
    "These metrics help determine how well the model generalizes to new, unseen data (i.e., test data).\n",
    "\n",
    "6.Training & Testing:\n",
    "Training refers to the process of feeding data into the model and adjusting its parameters based on the loss function and optimization algorithm.\n",
    "Testing involves evaluating the model's performance on unseen data (test data) to estimate how it will perform in real-world applications.\n",
    "\n",
    "7.Validation:\n",
    "Validation is the process of fine-tuning the model and hyperparameters using a validation set (a separate portion of the data) before testing the final model on the test data. Cross-validation techniques are often used to ensure the model's generalization performance.\n",
    "\n",
    "8.Hyperparameters:\n",
    "Hyperparameters are parameters that are set before the training process begins and control the learning process. These are not learned from the data but must be specified by the data scientist.\n",
    "Examples include the learning rate, number of hidden layers in a neural network, number of trees in a random forest, or the depth of a decision tree.\n",
    "Hyperparameter tuning is the process of finding the best combination of hyperparameters for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa7bc3b-bb53-40d9-b1fe-acf530d13487",
   "metadata": {},
   "source": [
    "Q4.How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "513aee05-a2af-4350-b17d-00991225ea84",
   "metadata": {},
   "source": [
    "The loss value (or loss function) is a critical metric in machine learning used to evaluate how well a model’s predictions match the actual target values (i.e., ground truth). It helps in determining whether the model is performing well or not by providing a quantitative measure of error. Here's how the loss value plays a key role in assessing and improving the model's performance:\n",
    "\n",
    "1. What is Loss?\n",
    "Loss is a measure of the difference between the predicted output of the model and the actual (true) value. The lower the loss, the better the model is at making predictions. The goal during training is typically to minimize the loss function, which in turn improves the model’s performance.\n",
    "The choice of loss function depends on the type of problem you're solving (e.g., classification, regression).\n",
    "\n",
    "2. How Does Loss Help Determine Model Performance?\n",
    "Quantifying Error: The loss value gives you a concrete number that quantifies the discrepancy between predicted and actual values. This helps you evaluate whether your model is making large errors or small ones.\n",
    "\n",
    "Guiding Optimization: During training, machine learning algorithms (like gradient descent) use the loss function to adjust the model’s parameters (weights) to reduce this error. The process of minimizing the loss function is central to model training.\n",
    "\n",
    "A high loss value indicates that the model’s predictions are far from the actual values.\n",
    "A low loss value indicates that the model is making predictions that are close to the actual values.\n",
    "\n",
    "3. Loss Function as a Performance Metric:\n",
    "Training vs Testing Loss:\n",
    "Training Loss refers to the error measured on the data used to train the model. A low training loss means the model has learned well from the training data.\n",
    "Testing Loss refers to the error measured on unseen data (validation or test data). A low testing loss indicates that the model generalizes well to new data, while a high testing loss suggests overfitting (where the model has memorized the training data and performs poorly on unseen data).\n",
    "Overfitting and Underfitting:\n",
    "Overfitting occurs when the model learns the training data too well, including noise and outliers. The loss on the training data will be very low, but the loss on the test data will be high because the model fails to generalize to new, unseen examples.\n",
    "Underfitting occurs when the model is too simple or too weak to capture the underlying patterns in the data. Both the training and testing loss will be high in this case.\n",
    "\n",
    "4. When Loss Alone Isn't Enough:\n",
    "Loss vs. Generalization: A low loss on the training set alone doesn't necessarily mean the model is good. A model might perform poorly on the test data even if the training loss is low (overfitting). Therefore, both training and testing loss should be considered.\n",
    "\n",
    "Other Metrics: Depending on the problem, you may also want to look at other metrics, such as accuracy, precision, recall, F1 score (for classification), or R² (R-squared) (for regression). Sometimes, loss alone does not capture the full picture, especially in imbalanced datasets or tasks where prediction errors have different consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30583419-4c5a-4b71-91c4-5bf668a1dcac",
   "metadata": {},
   "source": [
    "Q5.What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d336a6c-3c9c-4edf-a97c-6d18f7d75f9b",
   "metadata": {},
   "source": [
    "\n",
    "In data analysis and machine learning, variables can be classified into different types based on the nature of the data they represent. Two fundamental categories of variables are continuous variables and categorical variables. Here's a breakdown of both:\n",
    "\n",
    "1. Continuous Variables\n",
    "Definition:\n",
    "\n",
    "Continuous variables (also known as quantitative variables) are numerical variables that can take on an infinite number of values within a given range. These values are measurable and can be represented with real numbers, meaning they can have decimals, fractions, or whole numbers.\n",
    "The key characteristic of continuous variables is that there is an infinite number of possible values between any two points.\n",
    "Examples:\n",
    "\n",
    "Height: Can be 5.5 feet, 5.55 feet, 5.555 feet, etc.\n",
    "Weight: Can be 70 kg, 70.1 kg, 70.01 kg, and so on.\n",
    "Temperature: Can be 20°C, 20.1°C, 20.123°C, etc.\n",
    "Time: Can be 3 hours, 3.5 hours, 3.54 hours, etc.\n",
    "Income: Can be $30,000, $30,001, $30,000.50, etc.\n",
    "Key Characteristics:\n",
    "\n",
    "Continuous variables are measured, not counted.\n",
    "They can take on any value within a defined range.\n",
    "Commonly analyzed using mean, median, standard deviation, and other numerical statistics.\n",
    "Often visualized with histograms, line graphs, or scatter plots.\n",
    "\n",
    "\n",
    "2. Categorical Variables\n",
    "Definition:\n",
    "Categorical variables (also known as qualitative variables) are variables that represent categories or groups. These variables contain a limited number of distinct, non-numeric categories or labels, and the values are typically not ordered or have no inherent numerical meaning.\n",
    "Categorical variables can be further divided into two types: nominal and ordinal.\n",
    "Types of Categorical Variables:\n",
    "\n",
    "Nominal Variables: These are categorical variables where the values represent distinct groups or categories, and there is no meaningful order between them.\n",
    "\n",
    "Examples:\n",
    "Gender (Male, Female, Non-Binary)\n",
    "Colors (Red, Blue, Green)\n",
    "Cities (New York, Los Angeles, Chicago)\n",
    "Brands (Nike, Adidas, Puma)\n",
    "Ordinal Variables: These are categorical variables where the values represent distinct categories with a meaningful order or ranking, but the distance between the categories is not defined or consistent.\n",
    "\n",
    "Examples:\n",
    "Education level (High School, Bachelor's, Master's, Ph.D.)\n",
    "Customer satisfaction (Very Unsatisfied, Unsatisfied, Neutral, Satisfied, Very Satisfied)\n",
    "Rating scales (1 star, 2 stars, 3 stars, 4 stars, 5 stars)\n",
    "Key Characteristics:\n",
    "\n",
    "Categorical variables are counted, not measured.\n",
    "They are represented by categories or groups, not continuous values.\n",
    "Can be analyzed using frequency counts, mode, and proportions.\n",
    "Often visualized using bar charts, pie charts, or stacked bar charts.\n",
    "Ordinal categorical variables have a natural order (ranking), but nominal variables do not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622aaf6-42dc-4288-a90e-a96013e44893",
   "metadata": {},
   "source": [
    "Q6.How do we handle categorical variables in Machine Learning? What are the common t\n",
    "echniques?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2249a30-6953-4781-8a91-4458ee46a09b",
   "metadata": {},
   "source": [
    "\n",
    "Handling categorical variables in machine learning is a crucial step, as most machine learning algorithms require numeric inputs. Since categorical variables represent distinct categories or groups (such as gender, city, product type), they need to be transformed into numerical representations before they can be fed into the model. There are several techniques for handling categorical variables, each suited for different types of machine learning algorithms and problem contexts.\n",
    "\n",
    "Here are the most common techniques for handling categorical variables:\n",
    "\n",
    "1. One-Hot Encoding\n",
    "One-hot encoding is one of the most widely used methods for converting categorical variables into a binary (0 or 1) matrix. Each category of a categorical variable is represented as a separate binary column, where the value is 1 if the observation belongs to that category, and 0 otherwise.\n",
    "Example: If we have a categorical feature Color with categories {Red, Green, Blue}, one-hot encoding would convert it into three columns: Color_Red, Color_Green, Color_Blue. A red item would be represented as [1, 0, 0], a green item as [0, 1, 0], and so on.\n",
    "Advantages:\n",
    "\n",
    "Simple and widely supported by most machine learning algorithms.\n",
    "Avoids the assumption of any ordinal relationship between categories.\n",
    "Disadvantages:\n",
    "\n",
    "For categorical variables with many levels (high cardinality), one-hot encoding can lead to a large number of columns, resulting in sparse matrices and high dimensionality.\n",
    "It may not capture relationships between categories effectively (e.g., ordinal relationships)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b162e2f-3360-4aa4-8929-6c0ebc11f13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color_Blue  Color_Green  Color_Red\n",
      "0       False        False       True\n",
      "1       False         True      False\n",
      "2        True        False      False\n",
      "3       False         True      False\n",
      "4       False        False       True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'Color': ['Red', 'Green', 'Blue', 'Green', 'Red']})\n",
    "df_encoded = pd.get_dummies(df, columns=['Color'])\n",
    "print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85ac243b-0808-4220-957c-f0eb72af4b9c",
   "metadata": {},
   "source": [
    "2. Label Encoding\n",
    "Label encoding is a technique where each category of a categorical variable is assigned a unique integer value. This method is best suited for ordinal variables (i.e., variables where there is a meaningful order or ranking between categories).\n",
    "\n",
    "Example: If we have a categorical feature Size with categories {Small, Medium, Large}, label encoding would assign values as follows: Small = 0, Medium = 1, Large = 2.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simple and requires less memory compared to one-hot encoding.\n",
    "Works well for ordinal data where the numerical encoding maintains the inherent order of categories.\n",
    "Disadvantages:\n",
    "\n",
    "For nominal data (i.e., data where no inherent order exists), label encoding may introduce unintended ordinal relationships. For example, encoding Color as Red = 0, Green = 1, and Blue = 2 would create a false order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c930f802-28af-4678-8ca5-a7b176a7c8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Size  Size_encoded\n",
      "0   Small             2\n",
      "1  Medium             1\n",
      "2   Large             0\n",
      "3  Medium             1\n",
      "4   Small             2\n",
      "5   Large             0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "data = {'Size': ['Small', 'Medium', 'Large', 'Medium', 'Small', 'Large']}\n",
    "df = pd.DataFrame(data)\n",
    "le = LabelEncoder()\n",
    "df['Size_encoded'] = le.fit_transform(df['Size'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f0b49a8-a750-4d03-9a3b-c3bd79715656",
   "metadata": {},
   "source": [
    "3. Ordinal Encoding\n",
    "Ordinal encoding is similar to label encoding, but it specifically applies to ordinal categorical variables (where the categories have a meaningful order). This encoding maintains the order between categories by assigning increasing integer values based on the rank or hierarchy of the categories.\n",
    "\n",
    "Example: For a Satisfaction feature with categories {Very Low, Low, Medium, High, Very High}, you would encode them as: Very Low = 0, Low = 1, Medium = 2, High = 3, Very High = 4.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Keeps the order intact and works well for ordinal variables.\n",
    "Disadvantages:\n",
    "\n",
    "If the ordering is not strictly meaningful, it might introduce biases (e.g., assigning 3 to \"Medium\" and 2 to \"Low\" implies a false relationship)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3dc069-b323-431a-8246-ff89a4071319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Size  Size_encoded\n",
      "0   Small             0\n",
      "1  Medium             1\n",
      "2   Large             2\n",
      "3  Medium             1\n",
      "4   Small             0\n",
      "5   Large             2\n"
     ]
    }
   ],
   "source": [
    "size_order = {'Small': 0, 'Medium': 1, 'Large': 2}\n",
    "df['Size_encoded'] = df['Size'].map(size_order)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370835b5-34c0-44c4-828f-4eba2948b458",
   "metadata": {},
   "source": [
    "Q7.What do you mean by training and testing a dataset?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "056da4d4-9de4-4351-ae15-bf7d06c4bf62",
   "metadata": {},
   "source": [
    "In machine learning, the terms training and testing a dataset refer to the process of splitting your data into two distinct parts: one for training the model and the other for evaluating its performance. This division helps assess how well your model generalizes to new, unseen data, which is a key aspect of building effective machine learning models. Let's break down the concepts of training and testing in more detail:\n",
    "\n",
    "1. Training a Dataset\n",
    "Training a dataset means using a portion of the available data to \"teach\" the machine learning model. The model learns patterns, relationships, and structures from this data. During training, the model adjusts its parameters (e.g., weights in a neural network) to minimize the error or loss function, based on the inputs and their corresponding targets (labels in supervised learning).\n",
    "\n",
    "Steps in Training:\n",
    "Input: You feed the model the training data, which consists of input features and their corresponding target labels.\n",
    "Learning: The model learns by adjusting its parameters to minimize the difference between its predictions and the actual values (the target labels) using an optimization algorithm (e.g., gradient descent).\n",
    "Iteration: Training is typically done over multiple passes through the data (epochs), and during each iteration, the model improves its ability to predict.\n",
    "Example:\n",
    "Consider a dataset of housing prices, where the features might be square footage, number of rooms, and location, and the target is the price of the house. The model will \"learn\" how these features are related to the house price during training.\n",
    "\n",
    "2. Testing a Dataset\n",
    "Testing a dataset means evaluating the model’s performance on a separate portion of the data that the model has never seen during training. The goal is to assess how well the model can generalize to new, unseen data and predict correctly for real-world scenarios.\n",
    "\n",
    "Steps in Testing:\n",
    "Input: The model is given a testing set, which contains data it has not seen during the training phase. This dataset also includes known target labels.\n",
    "Evaluation: The model makes predictions based on the features of the test set. The predicted values are compared to the true target values to compute performance metrics like accuracy, precision, recall, F1-score, or mean squared error (MSE), depending on the type of problem (classification or regression).\n",
    "Model Assessment: Testing gives you an estimate of how well the model will perform on real-world, unseen data.\n",
    "Example:\n",
    "After training the model on the housing dataset, you would evaluate its performance by testing it on a separate test set. For instance, you might want to see how well it predicts house prices for new data it hasn't been trained on.\n",
    "\n",
    "The Train-Test Split\n",
    "The process of splitting your dataset into a training set and a testing set is called the train-test split. A common approach is to divide the data into 80% for training and 20% for testing, but the exact split ratio can vary based on the size of the dataset and the problem you're working on.\n",
    "\n",
    "Common Splits:\n",
    "80% Training, 20% Testing: This is a typical split.\n",
    "70% Training, 30% Testing: Used when more data is needed to test the model.\n",
    "90% Training, 10% Testing: Sometimes used with very large datasets.\n",
    "In addition to a simple train-test split, more advanced methods like cross-validation (discussed below) can be used for better model evaluation.\n",
    "\n",
    "Why Train and Test Separately?\n",
    "The main reason for separating the data into training and testing sets is to evaluate the model’s generalization ability. If the model is trained and tested on the same data, it could \"memorize\" the data, a problem known as overfitting. This leads to a situation where the model performs well on the training data but poorly on new, unseen data. Testing on a separate dataset ensures that the model is not just memorizing, but instead learning generalizable patterns.\n",
    "\n",
    "Key Points:\n",
    "Training: The model learns from the data and adjusts its parameters.\n",
    "Testing: The model’s performance is evaluated on unseen data to check how well it generalizes.\n",
    "Cross-Validation\n",
    "In addition to the simple train-test split, a more robust approach is cross-validation, which involves splitting the dataset into multiple parts and rotating through them. A common technique is k-fold cross-validation, where the data is split into k equally sized folds:\n",
    "\n",
    "Train the model on k-1 folds and test it on the remaining fold.\n",
    "Repeat this for each of the k folds.\n",
    "Average the performance scores to get a more reliable estimate of the model’s ability to generalize.\n",
    "Cross-validation helps in cases where the dataset is small, or you want a more reliable estimate of the model’s performance, as it reduces the variance that can occur due to a single split.\n",
    "\n",
    "Why Split Data?\n",
    "Avoid Overfitting: If the model has seen all the data, it could overfit and perform poorly on unseen data.\n",
    "Evaluate Generalization: Testing on unseen data gives an estimate of how the model will perform in real-world scenarios.\n",
    "Model Tuning: The training data is used to tune model parameters (like weights or hyperparameters), while the test data ensures that these settings result in a model that generalizes well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ddc87ff-31b4-4bae-a9b6-d43e87aac174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (X_train):\n",
      "   Feature1  Feature2\n",
      "4         5         1\n",
      "2         3         3\n",
      "0         1         5\n",
      "3         4         2\n",
      "\n",
      "Test Data (X_test):\n",
      "   Feature1  Feature2\n",
      "1         2         4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = {'Feature1': [1, 2, 3, 4, 5],\n",
    "        'Feature2': [5, 4, 3, 2, 1],\n",
    "        'Target': [0, 1, 0, 1, 0]}\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['Feature1', 'Feature2']]\n",
    "y = df['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Training Data (X_train):\")\n",
    "print(X_train)\n",
    "print(\"\\nTest Data (X_test):\")\n",
    "print(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575de41-ded1-4b08-b1b1-7594bfe6b531",
   "metadata": {},
   "source": [
    "Q8.What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02a72fc0-d47c-47ed-a197-533cb743846f",
   "metadata": {},
   "source": [
    "The sklearn.preprocessing module is a subpackage in scikit-learn (often referred to as sklearn) that provides various tools and techniques for preprocessing data before feeding it into machine learning algorithms. Preprocessing is a critical step in machine learning pipelines, as it involves transforming the raw data into a format that can be effectively used by machine learning models.\n",
    "\n",
    "The primary focus of sklearn.preprocessing is to handle tasks such as scaling, encoding, and normalizing data, which are essential for the proper functioning of many machine learning algorithms. This module contains classes and functions for:\n",
    "\n",
    "Scaling and Normalization: Adjusting the range or distribution of data.\n",
    "Encoding Categorical Variables: Converting categorical variables into numerical representations.\n",
    "Imputing Missing Values: Handling missing data appropriately.\n",
    "Polynomial Features: Generating new features that are polynomials of existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9097de7-1919-4f78-9f94-082d91a6b346",
   "metadata": {},
   "source": [
    "Q9.What is a Test set?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b0eef9c-f328-41a5-b3ef-dfdefdcf2d5c",
   "metadata": {},
   "source": [
    "A test set is a subset of the data that is used to evaluate the performance of a machine learning model after it has been trained. It is an essential part of the machine learning workflow and helps ensure that the model generalizes well to new, unseen data. The test set is kept separate from the data used for training the model, and the model does not see this data during the training phase.\n",
    "\n",
    "Key Concepts about a Test Set\n",
    "Purpose:\n",
    "The main purpose of a test set is to provide an unbiased evaluation of a trained model’s performance. It acts as a proxy for how the model will perform on real-world, unseen data. By testing on data that the model hasn't seen before, you can assess how well the model generalizes to new inputs.\n",
    "\n",
    "Evaluation:\n",
    "After training a machine learning model on the training set, the model is evaluated using the test set. This evaluation can involve various performance metrics, depending on the type of task:\n",
    "\n",
    "For classification tasks: metrics like accuracy, precision, recall, F1-score, and AUC-ROC.\n",
    "For regression tasks: metrics like mean squared error (MSE), mean absolute error (MAE), and R-squared.\n",
    "Unseen Data:\n",
    "The test set consists of data points that the model has never seen during the training phase. The purpose is to simulate real-world situations where the model must deal with data that it has not encountered before.\n",
    "\n",
    "No Data Leakage:\n",
    "It is critical that there is no leakage of information from the test set into the training process. This ensures that the model is tested on genuinely unseen data and gives an honest estimate of its performance.\n",
    "\n",
    "How is the Test Set Used?\n",
    "Training Phase:\n",
    "During training, the model learns patterns from the training set. The model’s parameters (like weights, if using neural networks) are adjusted based on the training data.\n",
    "\n",
    "Evaluation Phase:\n",
    "After training, the model is evaluated on the test set to see how well it performs. The test set is used only for evaluation and not for adjusting the model.\n",
    "\n",
    "Model Tuning:\n",
    "If the model’s performance on the test set is not satisfactory, you can modify the model (e.g., tuning hyperparameters, choosing a different algorithm, or applying additional preprocessing). However, the test set remains untouched during this process to ensure unbiased evaluation.\n",
    "\n",
    "Performance Metrics:\n",
    "Depending on the task, you compute various performance metrics on the test set:\n",
    "\n",
    "For classification: Accuracy, precision, recall, F1-score, confusion matrix, etc.\n",
    "For regression: MSE, RMSE (root mean square error), R², etc.\n",
    "Test Set Size\n",
    "The size of the test set is typically a percentage of the total dataset. Commonly, a dataset is split into:\n",
    "\n",
    "70-80% for training\n",
    "20-30% for testing\n",
    "However, the exact split can vary depending on the size of the dataset and specific requirements. If you have a large dataset, you might allocate 90% for training and 10% for testing. If the dataset is small, you might need a larger test set to get reliable performance estimates.\n",
    "\n",
    "Test Set vs. Validation Set\n",
    "While the test set is used to evaluate the final model, a validation set is sometimes used during the training process to tune hyperparameters (e.g., learning rate, number of layers, etc.). The validation set is used to:\n",
    "\n",
    "Select the best model (e.g., choosing the model that performs best on the validation set).\n",
    "Tune hyperparameters without using the test set.\n",
    "Once the model is fully trained and the hyperparameters are tuned using the training and validation sets, the final evaluation of the model is done on the test set to simulate its performance on unseen data\n",
    "\n",
    "Why is a Test Set Important?\n",
    "Generalization:\n",
    "The test set provides a realistic estimate of how well the model will perform in production, with data it hasn't seen before.\n",
    "\n",
    "Prevent Overfitting:\n",
    "If a model is tested on the same data it was trained on, it might simply memorize the data (overfitting). The test set helps prevent this by ensuring the model is evaluated on new, unseen data.\n",
    "\n",
    "Model Selection:\n",
    "It helps in comparing different models or configurations to select the one that performs best on unseen data.\n",
    "\n",
    "Avoid Bias:\n",
    "By using a separate test set, we avoid the risk of bias in evaluating the model's performance. This ensures the results reflect the model's ability to generalize.\n",
    "\n",
    "Key Takeaways about a Test Set\n",
    "A test set is a portion of the dataset used to evaluate the model after it has been trained.\n",
    "It is essential to keep the test set separate from the training and validation sets to avoid data leakage and ensure unbiased performance estimation.\n",
    "The test set is used to simulate real-world performance and helps in evaluating the model’s generalization capability.\n",
    "Performance metrics (accuracy, precision, recall, etc.) are calculated using the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90259c-c811-42f1-ac93-fbc7210ac407",
   "metadata": {},
   "source": [
    "Q10.How do we split data for model fitting (training and testing) in Python?\n",
    "How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "423dd92f-a134-4ea1-8726-8cd1f72cf637",
   "metadata": {},
   "source": [
    "How to Split Data for Model Fitting (Training and Testing) in Python?\n",
    "In Python, you typically use scikit-learn (sklearn), a popular machine learning library, to split data into training and test sets. The most common method to split data is by using train_test_split() from sklearn.model_selection. This function helps you divide your dataset into two parts: one for training the model and one for testing the model's performance.\n",
    "\n",
    "Steps to Split Data:\n",
    "Import Required Libraries:\n",
    "First, you need to import the necessary libraries and functions.\n",
    "\n",
    "Load or Create Dataset:\n",
    "You can either load an existing dataset (e.g., from pandas or sklearn.datasets) or create a custom dataset.\n",
    "\n",
    "Use train_test_split:\n",
    "Use train_test_split() to split your dataset into training and testing sets. You can also specify the size of the test set using the test_size parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c82d62-265f-46c9-ae43-5f07fffc774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      "   Feature1  Feature2\n",
      "4         5         1\n",
      "2         3         3\n",
      "0         1         5\n",
      "3         4         2\n",
      "\n",
      "Test Features:\n",
      "   Feature1  Feature2\n",
      "1         2         4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "data = {'Feature1': [1, 2, 3, 4, 5],\n",
    "        'Feature2': [5, 4, 3, 2, 1],\n",
    "        'Target': [0, 1, 0, 1, 0]}\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['Feature1', 'Feature2']]\n",
    "y = df['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Training Features:\")\n",
    "print(X_train)\n",
    "print(\"\\nTest Features:\")\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7064e975-a185-41e0-863f-7650628ff11a",
   "metadata": {},
   "source": [
    "How to Approach a Machine Learning Problem?\n",
    "Approaching a machine learning (ML) problem requires a structured and systematic process. Below is a typical step-by-step approach to solve ML problems:\n",
    "\n",
    "1. Define the Problem\n",
    "The first step is to clearly define the problem you're trying to solve. This will guide the entire process, including the type of model to use.\n",
    "\n",
    "Is it a classification problem? (e.g., spam detection, sentiment analysis)\n",
    "Is it a regression problem? (e.g., predicting house prices)\n",
    "Is it a clustering problem? (e.g., customer segmentation)\n",
    "Is it an anomaly detection problem? (e.g., fraud detection)\n",
    "\n",
    "2. Collect and Explore the Data\n",
    "Once you understand the problem, gather the relevant data. Data can come from multiple sources, such as databases, APIs, CSV files, or scraping from websites. Once the data is collected, perform exploratory data analysis (EDA) to understand it better:\n",
    "\n",
    "Load the data into a dataframe (e.g., using pandas).\n",
    "Visualize the data to check distributions and relationships between variables (e.g., using matplotlib, seaborn).\n",
    "Check for missing values, duplicates, or any anomalies in the data.\n",
    "Analyze the target variable (e.g., check its distribution for classification or regression tasks).\n",
    "\n",
    "3. Preprocess the Data\n",
    "Before applying machine learning algorithms, data often needs to be preprocessed:\n",
    "\n",
    "Handle Missing Values: Fill missing values using mean, median, mode, or use imputation methods.\n",
    "Encode Categorical Variables: Use methods like Label Encoding or One-Hot Encoding to convert categorical variables into numerical ones.\n",
    "Scale the Data: Normalize or standardize the features if they are on different scales (e.g., using StandardScaler or MinMaxScaler).\n",
    "Split the Data: Split the data into training and testing sets (as shown earlier) using train_test_split(). Alternatively, you can use cross-validation for more robust evaluation.\n",
    "\n",
    "4. Choose a Model\n",
    "Select an appropriate machine learning algorithm based on the problem type:\n",
    "\n",
    "For Classification: Logistic Regression, Decision Trees, Random Forests, Support Vector Machines, K-Nearest Neighbors (KNN), etc.\n",
    "For Regression: Linear Regression, Decision Trees, Random Forest Regressor, Support Vector Regression, etc.\n",
    "For Clustering: K-Means, DBSCAN, Hierarchical Clustering, etc.\n",
    "For Time Series: ARIMA, LSTM (for deep learning), Prophet, etc.\n",
    "The choice of model depends on factors like data size, complexity, interpretability, and computational resources.\n",
    "\n",
    "5. Train the Model\n",
    "Once you've chosen a model, fit it to your training data:\n",
    "\n",
    "6. Evaluate the Model\n",
    "After training, evaluate the model's performance using the test data (unseen data). This step involves comparing the predicted outputs to the true outputs to assess how well the model performs.\n",
    "\n",
    "For Classification: Accuracy, Precision, Recall, F1-score, Confusion Matrix, ROC-AUC curve.\n",
    "For Regression: Mean Squared Error (MSE), R-squared, Mean Absolute Error (MAE).\n",
    "\n",
    "7. Tune the Model\n",
    "Model tuning involves improving the model’s performance by adjusting hyperparameters and improving data features. Techniques include:\n",
    "\n",
    "Hyperparameter Tuning: Use GridSearchCV or RandomizedSearchCV to find the best combination of hyperparameters.\n",
    "Feature Engineering: Add new features, remove irrelevant ones, or perform dimensionality reduction (e.g., PCA).\n",
    "Ensemble Methods: Combine multiple models to improve performance (e.g., boosting, bagging).\n",
    "\n",
    "8. Interpret the Results\n",
    "Interpreting the model helps in understanding how it makes predictions. This step is crucial in real-world applications, especially in domains like healthcare, finance, and legal systems.\n",
    "\n",
    "Feature Importance: For tree-based models, check which features are most important.\n",
    "Model Explainability: Use techniques like SHAP or LIME for explaining predictions, especially in complex models like deep neural networks.\n",
    "\n",
    "9. Deployment\n",
    "Once the model is trained and evaluated, it’s time to deploy it for real-world use:\n",
    "\n",
    "Save the Model: Save the trained model using joblib or pickle to make it available for future predictions.\n",
    "Deploy the Model: Integrate the model into a production environment, which could involve using frameworks like Flask, FastAPI, or cloud platforms like AWS, GCP, or Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158153d-bb59-4df1-9dbc-e320c1dac3be",
   "metadata": {},
   "source": [
    "Q11.Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d5a99f8-caff-4062-906f-2014c45f426c",
   "metadata": {},
   "source": [
    "Performing Exploratory Data Analysis (EDA) before fitting a machine learning model to the data is a crucial step in the data science and machine learning workflow. EDA helps you understand the dataset, identify patterns, relationships, and potential issues that might affect the performance of the model. Without performing EDA, you risk building a model that is ineffective, inefficient, or even biased.\n",
    "\n",
    "Here are some key reasons why EDA is essential before fitting a model:\n",
    "\n",
    "1. Understand the Structure and Nature of the Data\n",
    "EDA helps you gain a clear understanding of the dataset you're working with:\n",
    "\n",
    "Types of Variables: Identifying the types of variables (numerical, categorical, ordinal) is important to decide the type of model and preprocessing steps required.\n",
    "Shape of the Data: It helps to check the number of features and the number of samples. For example, a model that works well for small datasets might not perform well with large datasets, and vice versa.\n",
    "2. Detect and Handle Missing Data\n",
    "Missing data is common in real-world datasets, and its handling is essential for building reliable models.\n",
    "\n",
    "Identify Missing Values: During EDA, you can spot missing values in the dataset (e.g., using .isnull() in Pandas).\n",
    "Handle Missing Data: Depending on the extent and nature of missing values, you can either:\n",
    "Impute missing values (mean, median, or mode imputation, or using advanced imputation methods).\n",
    "Remove rows/columns with missing values if imputation is not appropriate.\n",
    "Not handling missing values correctly can lead to biased models or errors during model training.\n",
    "\n",
    "3. Detect Outliers and Anomalies\n",
    "Outliers or anomalies can significantly affect the performance of many machine learning algorithms (e.g., regression, k-NN).\n",
    "\n",
    "Visualize Data: Using box plots, scatter plots, or histograms, EDA helps to detect extreme values or anomalies.\n",
    "Handle Outliers: Once identified, you can decide how to handle outliers — either by removing them, capping, or transforming them based on domain knowledge or model requirements.\n",
    "For example, regression models are sensitive to outliers, and failing to address them might lead to overfitting or incorrect conclusions.\n",
    "\n",
    "4. Understand Data Distributions\n",
    "Knowing the distribution of data helps to choose the right algorithms and preprocessing techniques.\n",
    "\n",
    "Check for Normality: Many algorithms (e.g., Linear Regression, Naive Bayes) assume that the data follows a Gaussian (normal) distribution. If the data is skewed or has a heavy tail, you might need to transform it (e.g., using logarithmic transformation).\n",
    "Visualize Distributions: Use histograms, KDE (Kernel Density Estimation) plots, and QQ plots to examine the distribution of features.\n",
    "For instance, if you are working with algorithms that assume normally distributed data (e.g., linear regression), it’s good practice to check and transform the data if necessary.\n",
    "\n",
    "5. Identify Relationships Between Features\n",
    "EDA allows you to explore the relationships between different features, which can provide valuable insights into how the features relate to each other.\n",
    "\n",
    "Correlation Matrix: By calculating correlation coefficients (e.g., Pearson’s, Spearman’s), you can identify whether some features are highly correlated. Highly correlated features can lead to multicollinearity issues in regression models.\n",
    "Visualize Relationships: Scatter plots, pair plots, and correlation matrices are helpful in visualizing the relationships between continuous variables. This is particularly useful for linear or logistic regression models where feature relationships influence the model's performance.\n",
    "Understanding relationships helps in:\n",
    "\n",
    "Feature Engineering: Creating new features based on these relationships.\n",
    "Removing Redundant Features: Dropping highly correlated features to avoid multicollinearity.\n",
    "6. Check for Class Imbalance (in Classification Problems)\n",
    "In classification tasks, EDA helps you to check whether the classes are balanced.\n",
    "\n",
    "Class Distribution: If the target variable (label) is imbalanced (e.g., 95% of one class and 5% of another), it can severely affect model performance, leading to biased predictions.\n",
    "Methods to Handle Imbalance: During EDA, you can decide to handle class imbalance using techniques like:\n",
    "Resampling (undersampling or oversampling).\n",
    "Using weighted loss functions.\n",
    "Synthetic data generation (e.g., SMOTE).\n",
    "For example, in a binary classification task where 95% of the data belongs to Class A and only 5% to Class B, an imbalanced dataset can result in the model predicting only Class A, with poor predictive performance for Class B.\n",
    "\n",
    "7. Feature Engineering and Transformation\n",
    "EDA helps you understand which features are most important and which might need to be transformed.\n",
    "\n",
    "Create New Features: Based on the data exploration, you might discover ways to create new features that enhance the predictive power of the model (e.g., combining multiple columns, creating interaction terms, extracting date components).\n",
    "Feature Scaling: Some algorithms (e.g., SVM, KNN, Logistic Regression) perform better when features are scaled or normalized. EDA helps you recognize if scaling is required.\n",
    "For example, in time series data, EDA may lead you to create features such as month, day of the week, or time lags to help improve the model's predictive ability.\n",
    "\n",
    "8. Detecting Duplicate Data\n",
    "Sometimes, datasets contain duplicate rows which can lead to biased or incorrect models.\n",
    "\n",
    "Check for Duplicates: During EDA, you can use functions like .duplicated() in Pandas to identify duplicate rows.\n",
    "Remove Duplicates: If necessary, you can drop duplicates to ensure that the model is not learning from redundant information.\n",
    "9. Identify the Right Model Approach\n",
    "By performing EDA, you get insights into the structure of the data, which guides your choice of machine learning model:\n",
    "\n",
    "If the data is linearly separable, linear models (e.g., Logistic Regression) may work well.\n",
    "If there are complex relationships, non-linear models (e.g., Decision Trees, Random Forests, or Neural Networks) might be more appropriate.\n",
    "If the data is highly imbalanced, you may opt for algorithms that handle class imbalance well (e.g., XGBoost or Random Forest with class weighting).\n",
    "EDA helps you identify whether a model is appropriate or if the problem requires further feature engineering or data transformations.\n",
    "\n",
    "10. Validate Assumptions\n",
    "Many machine learning algorithms make certain assumptions about the data. For example:\n",
    "\n",
    "Linear Regression assumes a linear relationship between the features and the target variable.\n",
    "Logistic Regression assumes that the target variable is categorical and that there is a linear relationship between the independent variables and the log odds of the target.\n",
    "By performing EDA, you can check if these assumptions hold true in the data:\n",
    "\n",
    "Linearity: Check if there’s a linear relationship between features and the target.\n",
    "Independence: Ensure that features are independent (no multicollinearity) when required by the model.\n",
    "Homoscedasticity: Ensure that the variance of errors is constant (especially for regression models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00737e20-5d54-4171-8deb-d5e92b9935e9",
   "metadata": {},
   "source": [
    "Q12.What is correlation?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df92671f-e928-4594-b756-7340f95d5b0e",
   "metadata": {},
   "source": [
    "Correlation is a statistical measure that describes the strength and direction of a relationship between two or more variables. In simple terms, it shows how one variable changes when another variable changes.\n",
    "\n",
    "Example of Correlation:\n",
    "1.Positive Correlation: The number of hours worked and salary are often positively correlated. As the number of hours worked increases, salary tends to increase.\n",
    "2.Negative Correlation: The number of hours spent watching TV and physical fitness may have a negative correlation. As TV watching increases, physical activity tends to decrease.\n",
    "\n",
    "Types of Correlation:\n",
    "1.Pearson’s Correlation: Measures the linear relationship between two continuous variables.\n",
    "2.Spearman’s Rank Correlation: A non-parametric test that measures the monotonic relationship between two variables (whether or not the relationship is linear).\n",
    "3.Kendall’s Tau: Another non-parametric measure of correlation, similar to Spearman's rank, but generally used for smaller data sets.\n",
    "\n",
    "Visualizing Correlation:\n",
    "Scatter Plot: A scatter plot can be used to visually inspect the relationship between two variables. If the points tend to form a straight line, this suggests a strong linear correlation (positive or negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643789e-d9f2-4a95-8174-f88423114177",
   "metadata": {},
   "source": [
    "Q13.What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da384011-a884-42f1-bfea-9d41f158e478",
   "metadata": {},
   "source": [
    "Negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa. In statistical terms, two variables are negatively correlated if there is an inverse relationship between them. The strength and direction of this relationship are measured by the correlation coefficient, typically represented by Pearson's correlation coefficient (r).\n",
    "\n",
    "Pearson's r value for a negative correlation ranges from -1 to 0.\n",
    "A correlation of -1 indicates a perfect negative correlation.\n",
    "A correlation of 0 indicates no linear relationship between the variables.\n",
    "Interpretation of Negative Correlation:\n",
    "If the correlation between two variables is negative, it implies that when one variable increases, the other variable tends to decrease.\n",
    "Conversely, when one variable decreases, the other variable is likely to increase.\n",
    "Example of Negative Correlation:\n",
    "Height and Weight (up to a point): Generally, in a given population, as the height of a person increases, their weight might increase as well. However, in certain contexts (e.g., in very specific subgroups), one might see a negative correlation between height and weight, where increasing height might be associated with a decrease in weight (for example, among a group of people who are very tall but have a lean build).\n",
    "\n",
    "Temperature and Heating Costs: There is often a negative correlation between outdoor temperature and heating costs. As outdoor temperature increases, the need for heating (and thus heating costs) decreases. So, as temperature rises, heating costs tend to go down, showing a negative correlation.\n",
    "\n",
    "Mathematical Example:\n",
    "Suppose you have two variables: X and Y.\n",
    "\n",
    "X = hours spent studying\n",
    "Y = number of mistakes made on a test\n",
    "The relationship between these variables might show a negative correlation: as the number of hours spent studying increases (X increases), the number of mistakes made on a test (Y) decreases.\n",
    "\n",
    "If we calculate Pearson's correlation coefficient for this data and it comes out to be r = -0.85, this indicates a strong negative correlation: the more hours spent studying, the fewer mistakes made on the test.\n",
    "\n",
    "Real-World Examples of Negative Correlation:\n",
    "Speed and Travel Time: In a constant distance journey, the faster the speed, the less time it takes to travel. This is a negative correlation: as speed increases, travel time decreases.\n",
    "\n",
    "Education Level and Unemployment Rate: In many economies, a higher level of education tends to be correlated with a lower rate of unemployment. As education levels increase, unemployment rates tend to decrease.\n",
    "\n",
    "Stock Prices and Interest Rates: There can be a negative correlation between interest rates and stock prices. When interest rates rise, stocks might fall because higher borrowing costs reduce corporate profits and consumer spending.\n",
    "\n",
    "Graphical Representation of Negative Correlation:\n",
    "In a scatter plot, negative correlation is visualized by a downward-sloping trend. As one variable increases along the x-axis, the other variable decreases along the y-axis.\n",
    "\n",
    "Example:\n",
    "If the scatter plot of height vs. weight for a particular group of people shows a downward slope (high values of height correspond to low values of weight), this indicates a negative correlation.\n",
    "Summary of Negative Correla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622fdc95-460f-4e7e-8368-3dab586ae732",
   "metadata": {},
   "source": [
    "Q14.How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37648502-d3ce-4be6-a308-4ba05fda81f2",
   "metadata": {},
   "source": [
    "To find the correlation between variables in Python, we typically use the pandas library, which provides a simple way to compute correlation coefficients between columns in a DataFrame. Additionally, seaborn and matplotlib can be used for visualization to better understand the relationships between variables.\n",
    "\n",
    "Here's a step-by-step guide to calculate correlation between variables in Python:\n",
    "\n",
    "1. Using pandas to Compute Correlation\n",
    "The most common method to compute correlations between variables is using pandas.DataFrame.corr(). This function calculates the Pearson correlation coefficient by default, but it can also be used to compute other types of correlations such as Spearman or Kendall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b30fdbb-da42-4e85-85a4-3fd7e464dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Height    Weight       Age\n",
      "Height  1.000000  0.950255  0.706906\n",
      "Weight  0.950255  1.000000  0.626628\n",
      "Age     0.706906  0.626628  1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {\n",
    "    'Height': [5.5, 6.2, 5.8, 6.0, 5.9],\n",
    "    'Weight': [150, 180, 165, 170, 160],\n",
    "    'Age': [25, 30, 22, 28, 26]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "correlation_matrix = df.corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "292b2a0f-b8ab-4aef-833d-4c66e4981e14",
   "metadata": {},
   "source": [
    "2. Types of Correlation Methods in pandas.corr()\n",
    "By default, pandas.corr() computes the Pearson correlation, but it can also compute Spearman and Kendall correlations, which measure monotonic relationships (not necessarily linear).\n",
    "\n",
    "Pearson: Measures linear correlation. Values range from -1 to 1.\n",
    "Spearman: Measures rank correlation, i.e., how well the relationship between two variables can be described using a monotonic function.\n",
    "Kendall: Measures the strength of a monotonic relationship and is more robust to outliers.\n",
    "To specify a different correlation method, you can pass the method parameter to .corr():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc4297c6-d23f-4c65-aeef-98d6ac1258cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation:\n",
      "         Height  Weight  Age\n",
      "Height     1.0     0.9  0.9\n",
      "Weight     0.9     1.0  0.7\n",
      "Age        0.9     0.7  1.0\n",
      "Kendall Correlation:\n",
      "         Height  Weight  Age\n",
      "Height     1.0     0.8  0.8\n",
      "Weight     0.8     1.0  0.6\n",
      "Age        0.8     0.6  1.0\n"
     ]
    }
   ],
   "source": [
    "spearman_corr = df.corr(method='spearman')\n",
    "kendall_corr = df.corr(method='kendall')\n",
    "print(\"Spearman Correlation:\\n\", spearman_corr)\n",
    "print(\"Kendall Correlation:\\n\", kendall_corr)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c30a334-a470-4a8e-aa82-34c8d758ba61",
   "metadata": {},
   "source": [
    "3. Visualizing the Correlation Matrix Using seaborn\n",
    "A visual representation of the correlation matrix can provide better insights, especially when dealing with many variables. The seaborn library is commonly used for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bce7fa2-6759-4390-a535-cb7233813844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57cf79-c3f2-4592-8602-d928e222a619",
   "metadata": {},
   "source": [
    "Q15.What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9539ffce-3edf-42af-9621-f1f3eac9901a",
   "metadata": {},
   "source": [
    "What is Causation?\n",
    "Causation refers to a relationship where one event (the cause) directly influences or brings about another event (the effect). In other words, causation implies that one variable actually causes the change in another variable.\n",
    "\n",
    "For example, smoking causes lung cancer — this is a causal relationship because smoking increases the risk of developing lung cancer.\n",
    "\n",
    "Cause: Smoking\n",
    "Effect: Lung cancer\n",
    "In causal relationships, there is a direct influence of one variable on another. If variable A causes variable B, then changing A will directly lead to a change in B, often in a predictable manner.\n",
    "\n",
    "Correlation vs. Causation\n",
    "While both correlation and causation describe a relationship between variables, they are not the same. The key difference lies in the nature of that relationship:\n",
    "\n",
    "Correlation: Correlation indicates that two variables are related to each other, but it does not tell us anything about cause-and-effect. It simply describes how two variables move together (either positively or negatively) in a statistical sense. Correlation can be observed without one variable causing the other.\n",
    "\n",
    "Causation: Causation means that one variable directly influences the other. Causal relationships suggest that changes in one variable bring about changes in the other.\n",
    "\n",
    "Example to Illustrate the Difference\n",
    "Example 1: Correlation\n",
    "Observation: Studies show a positive correlation between ice cream sales and the number of drowning incidents.\n",
    "Correlation: As ice cream sales increase, drowning incidents also increase.\n",
    "Interpretation: This is a correlation because ice cream sales and drowning deaths move together, but this does not mean that ice cream causes drowning.\n",
    "\n",
    "Why?: The increase in both ice cream sales and drowning incidents likely happens during the summer months when the weather is hot. People are more likely to buy ice cream and go swimming at the same time, increasing both variables. This is an example of a third-variable problem (the hot weather) driving both outcomes.\n",
    "Example 2: Causation\n",
    "Observation: Studies have shown that smoking is a direct cause of lung cancer.\n",
    "Causation: There is sufficient medical and scientific evidence to prove that smoking directly leads to lung cancer.\n",
    "Interpretation: In this case, smoking causes lung cancer, which is a clear causal relationship.\n",
    "\n",
    "Why?: Extensive scientific studies and experiments have shown that chemicals in cigarette smoke damage the lungs over time, increasing the risk of developing cancer. This relationship is well-established and proven, which makes it a causal link.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681d528-1f03-445f-b8a4-cb92ba64e84c",
   "metadata": {},
   "source": [
    "Q16.What is an Optimizer? What are different types of optimizers? Explain each with an example"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e40397d0-5ead-4422-97dc-798d2d3e4147",
   "metadata": {},
   "source": [
    "What is an Optimizer in Machine Learning?\n",
    "An optimizer in machine learning is an algorithm or method used to minimize (or maximize) a loss function during training. The optimizer adjusts the weights or parameters of a model in such a way that the model's performance improves with respect to the data it is trained on. The goal of an optimizer is to find the best parameters (weights and biases) of the model that minimize the error or loss.\n",
    "\n",
    "Why Optimizers Are Important?\n",
    "In supervised learning, the model is trained to minimize the loss function, which quantifies how far off the model's predictions are from the actual target values. Optimizers control how the weights are updated by taking steps based on the gradients of the loss function with respect to the model parameters.\n",
    "\n",
    "Key Concept: Gradient Descent\n",
    "Most optimizers are based on gradient descent, which is a method to minimize the loss function by iteratively moving in the direction of the negative gradient of the function. The learning rate determines how large a step the optimizer takes in the direction of the gradient.\n",
    "\n",
    "Different Types of Optimizers\n",
    "1. Stochastic Gradient Descent (SGD)\n",
    "Description: SGD is one of the simplest and most commonly used optimizers. It updates the model's weights by computing the gradient of the loss function with respect to each training example and adjusting the weights based on this gradient.\n",
    "\n",
    "In batch gradient descent, the gradient is calculated over the entire dataset.\n",
    "In stochastic gradient descent, the gradient is computed using a single data point at each iteration, which makes it faster but more noisy.\n",
    "Steps:\n",
    "\n",
    "Compute the gradient of the loss function with respect to the parameters for each sample.\n",
    "Update the parameters by subtracting the product of the learning rate and the gradient.\n",
    "Formula:\n",
    "\n",
    "θ=θ−η⋅∇J(θ)\n",
    "θ are the model parameters.\n",
    "η is the learning rate.\n",
    "∇J(θ) is the gradient of the loss function.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Simple and computationally efficient.\n",
    "Can handle large datasets.\n",
    "Disadvantages:\n",
    "\n",
    "High variance and can converge slowly.\n",
    "Sensitive to the choice of the learning rate.\n",
    "2. Momentum Optimizer\n",
    "Description: Momentum is an extension to basic SGD that helps accelerate gradient descent in the relevant direction and dampens oscillations. It does this by adding a fraction of the previous update to the current update, allowing it to \"move faster\" in the direction of steepest descent.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Helps accelerate convergence.\n",
    "Reduces the oscillations seen in standard SGD.\n",
    "Disadvantages:\n",
    "\n",
    "Requires tuning of the momentum parameter.\n",
    "Might overshoot the optimal point if the momentum is too large.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f1128-4b61-47af-9ac0-85c8962e00ee",
   "metadata": {},
   "source": [
    "Q17.What is sklearn.linear_model ?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d3175ce-4d18-43a1-b307-6856a7d37b7d",
   "metadata": {},
   "source": [
    "What is sklearn.linear_model?\n",
    "sklearn.linear_model is a module in the scikit-learn library, which provides a collection of linear models for machine learning. Linear models are a family of algorithms that model the relationship between the input variables (features) and the output variable (target) using a linear equation. These models are based on linear assumptions and are commonly used in regression and classification tasks.\n",
    "\n",
    "The module provides various types of linear models that can be used for both supervised and unsupervised learning. These models are widely used because they are simple, interpretable, and computationally efficient.\n",
    "\n",
    "Common Linear Models in sklearn.linear_model\n",
    "1.Linear Regression (LinearRegression)\n",
    "\n",
    "Used for regression tasks (predicting a continuous target variable).\n",
    "Assumes a linear relationship between the input features and the target variable.\n",
    "The goal is to fit a linear equation (a straight line in 2D, or a hyperplane in higher dimensions) to the data.\n",
    "\n",
    "2.Ridge Regression (Ridge)\n",
    "\n",
    "A variant of linear regression that includes L2 regularization (penalty term) to reduce the complexity of the model and prevent overfitting.\n",
    "The penalty term shrinks the coefficients (weights) of less important features, making them approach zero but not exactly zero.\n",
    "\n",
    "3.Lasso Regression (Lasso)\n",
    "\n",
    "Another variant of linear regression that includes L1 regularization, which encourages sparsity (i.e., it forces some feature coefficients to become exactly zero).\n",
    "Lasso is particularly useful when you have many features and you want to perform feature selection by forcing irrelevant features to have zero coefficients.\n",
    "\n",
    "4.ElasticNet (ElasticNet)\n",
    "\n",
    "A linear regression model that combines both L1 (Lasso) and L2 (Ridge) regularization. It is useful when you have many correlated features.\n",
    "ElasticNet is particularly useful when the number of predictors is greater than the number of observations or when there is multicollinearity.\n",
    "\n",
    "Key Parameters of Linear Models\n",
    "alpha (regularization strength): In models like Ridge, Lasso, and ElasticNet, the alpha parameter controls the amount of regularization applied. Higher values of alpha correspond to stronger regularization.\n",
    "fit_intercept: Whether or not to fit an intercept (bias term) in the model. If set to False, the model will assume that the data is centered around the origin.\n",
    "max_iter: The maximum number of iterations for the optimization algorithm (useful in models like LogisticRegression and SGDClassifier).\n",
    "solver: Defines the optimization algorithm to be used (e.g., ‘lbfgs’, ‘liblinear’ for Logistic Regression).\n",
    "l1_ratio: In ElasticNet, this controls the mix between Lasso (L1) and Ridge (L2) regularization.\n",
    "Use Cases of Linear Models\n",
    "Regression Tasks:\n",
    "\n",
    "Predicting continuous values such as housing prices, stock market prices, or temperature.\n",
    "Examples: Linear Regression, Ridge Regression, Lasso Regression, ElasticNet.\n",
    "Classification Tasks:\n",
    "\n",
    "Predicting discrete class labels (binary or multi-class).\n",
    "Examples: Logistic Regression, RidgeClassifier, Perceptron, SGDClassifier.\n",
    "Feature Selection:\n",
    "\n",
    "Lasso Regression is often used for feature selection because it can shrink some feature coefficients to zero, effectively removing irrelevant features.\n",
    "Multicollinearity:\n",
    "\n",
    "Ridge and ElasticNet regression are useful in situations where features are highly correlated (multicollinearity), as they help stabilize the regression estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21c7d582-dd7f-4747-acb8-176d85bb4d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values: [2.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Predicted values:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e1ab63-3e18-4dcd-bfe1-be26a6d6d7fd",
   "metadata": {},
   "source": [
    "Q18.What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9a3a63c-63c5-4c97-98ce-cc3539fd52a6",
   "metadata": {},
   "source": [
    "What Does model.fit() Do?\n",
    "The model.fit() method is used to train a machine learning model on the provided data. It adjusts the parameters (such as weights) of the model to minimize a loss function, thereby making the model better at making predictions. This method essentially learns from the training data by finding patterns that allow the model to generalize to unseen data.\n",
    "\n",
    "In supervised learning, the training data consists of input features (X) and corresponding labels (y). The fit() function uses this data to learn the relationship between the features and the target labels.\n",
    "\n",
    "Arguments of model.fit()\n",
    "The arguments passed to model.fit() depend on the specific machine learning model (e.g., linear regression, classification, etc.), but the general form looks like this:\n",
    "model.fit(X_train, y_train, **kwargs)\n",
    "\n",
    "Here:\n",
    "\n",
    "X_train: The training data (input features) — typically a 2D array or matrix where each row represents a sample (example) and each column represents a feature (attribute).\n",
    "y_train: The target labels (output) — a 1D array or vector containing the target values corresponding to each training sample. For regression tasks, this is usually continuous values, and for classification tasks, this is categorical labels.\n",
    "\n",
    "Basic Arguments:\n",
    "X_train (required):\n",
    "\n",
    "A 2D array or matrix of shape (n_samples, n_features), where n_samples is the number of training examples and n_features is the number of features per sample.\n",
    "Example: If you're predicting housing prices based on size and number of rooms, X_train could have columns like square footage and number of rooms for each house.\n",
    "y_train (required):\n",
    "\n",
    "A 1D array (or a 2D array for multilabel problems) containing the labels (targets) corresponding to each row in X_train.\n",
    "Example: For the housing price prediction, y_train would contain the actual prices of the houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1622c8f8-bf0b-422a-8be3-5a2e25a1f669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LinearRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.array([1, 2, 3, 4, 5])\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3bcffd-696b-49bb-a867-e8972f316ba7",
   "metadata": {},
   "source": [
    "Q19.What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78e3b0e2-71a7-4521-8026-0ce1f3a4e416",
   "metadata": {},
   "source": [
    "What Does model.predict() Do?\n",
    "The model.predict() method is used to make predictions on new, unseen data after the model has been trained using model.fit(). It takes input features as arguments, processes them through the trained model, and outputs the predicted target values.\n",
    "\n",
    "For regression models, model.predict() returns continuous numerical values (predictions).\n",
    "For classification models, model.predict() returns the predicted class labels.\n",
    "The predictions are made based on the patterns and relationships that the model has learned during training.\n",
    "\n",
    "Arguments of model.predict()\n",
    "The main argument that must be provided to model.predict() is:\n",
    "\n",
    "X (required):\n",
    "A 2D array or matrix of shape (n_samples, n_features), where n_samples is the number of examples you want to make predictions for, and n_features is the number of features (or input variables) each example has.\n",
    "Important: X should have the same number of features as the training data that was used to fit the model (X_train), or else the model will not work properly.\n",
    "For example:\n",
    "\n",
    "For regression models (like LinearRegression, Ridge, etc.), the output will be a continuous numerical value for each input sample.\n",
    "For classification models (like LogisticRegression, SVM, RandomForestClassifier, etc.), the output will be the predicted class label(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f0e55b1-8eb7-4244-a54e-8cd61f742111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [6. 7.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.array([1, 2, 3, 4, 5])\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "X_test = np.array([[6], [7]])\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Predictions: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e44d29-b3df-4d0a-a3dd-ef9e7aaebdbe",
   "metadata": {},
   "source": [
    "Q20.What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36b857ef-d792-4811-8c0a-99972bbe57c8",
   "metadata": {},
   "source": [
    "Continuous and Categorical Variables\n",
    "In data science and machine learning, variables are the characteristics or features that are observed in a dataset. These variables can be classified into two primary types based on the nature of the data they represent: continuous variables and categorical variables.\n",
    "\n",
    "Continuous Variables\n",
    "Continuous variables are numerical variables that can take on any value within a given range. These values can be measured, and they often represent quantities that are on a continuous scale. Continuous variables are typically associated with real numbers and can have infinite possibilities within a certain range or interval.\n",
    "\n",
    "Characteristics:\n",
    "Can take any value within a range (e.g., fractions, decimals).\n",
    "Often measured on an interval or ratio scale.\n",
    "Represent quantitative data.\n",
    "Examples:\n",
    "Height: Could be 5.5 feet, 5.55 feet, 5.555 feet, etc.\n",
    "Weight: Could be 150.5 kg, 150.55 kg, 150.555 kg, etc.\n",
    "Temperature: Can be 30.1°C, 30.15°C, 30.155°C, etc.\n",
    "Age: 25.6 years, 25.61 years, etc.\n",
    "Income: A person’s annual salary, like 50,000.50 dollars.\n",
    "Key Points:\n",
    "Continuous variables are often used in regression tasks.\n",
    "They can take any value (integer or decimal) within a range, and the scale is meaningful.\n",
    "Examples of data types: Integer, Float, Double.\n",
    "\n",
    "\n",
    "Categorical Variables\n",
    "Categorical variables (also called nominal variables) represent data that can be divided into distinct categories or groups. The values of categorical variables are typically labels or names, which are often non-numeric but can sometimes be represented as numbers (e.g., \"1\" for \"male\" and \"2\" for \"female\"). The key feature of categorical variables is that the order or magnitude of the categories doesn't matter (except in ordinal variables, explained below).\n",
    "\n",
    "Characteristics:\n",
    "Can take on one of a limited, fixed number of possible values (called categories or levels).\n",
    "Usually represent qualitative data.\n",
    "Categories are mutually exclusive (i.e., each observation can belong to only one category).\n",
    "Examples:\n",
    "Gender: Categories like \"Male\" and \"Female\".\n",
    "Marital Status: Categories like \"Single\", \"Married\", \"Divorced\".\n",
    "Color: Categories like \"Red\", \"Blue\", \"Green\".\n",
    "Country: Categories like \"USA\", \"India\", \"Canada\".\n",
    "Product Type: Categories like \"Electronics\", \"Clothing\", \"Groceries\".\n",
    "Types of Categorical Variables:\n",
    "Nominal Variables:\n",
    "Nominal variables are unordered categories.\n",
    "The categories have no inherent order or ranking.\n",
    "Example: \"Red\", \"Blue\", \"Green\" (no intrinsic order).\n",
    "Ordinal Variables:\n",
    "Ordinal variables represent categories that have a meaningful order but no consistent difference between the categories.\n",
    "Example: \"Low\", \"Medium\", \"High\" (the order is important, but the difference between levels is not consistent).\n",
    "Key Points:\n",
    "Categorical variables are often used in classification tasks.\n",
    "Nominal categorical variables cannot be ranked or ordered, while ordinal variables can be.\n",
    "Categorical variables can be converted into numeric representations for machine learning using encoding techniques like One-Hot Encoding or Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eb18337-3415-4375-8f58-ecaf80321bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Data:\n",
      "   Age  Salary\n",
      "0   22   30000\n",
      "1   34   50000\n",
      "2   45   75000\n",
      "3   50  100000\n",
      "4   23   40000\n",
      "\n",
      "Categorical Data:\n",
      "   Gender Education\n",
      "0    Male  Bachelor\n",
      "1  Female    Master\n",
      "2  Female       PhD\n",
      "3    Male    Master\n",
      "4  Female  Bachelor\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_continuous = {\n",
    "    'Age': [22, 34, 45, 50, 23],\n",
    "    'Salary': [30000, 50000, 75000, 100000, 40000]\n",
    "}\n",
    "\n",
    "df_continuous = pd.DataFrame(data_continuous)\n",
    "print(\"Continuous Data:\")\n",
    "print(df_continuous)\n",
    "\n",
    "data_categorical = {\n",
    "    'Gender': ['Male', 'Female', 'Female', 'Male', 'Female'],\n",
    "    'Education': ['Bachelor', 'Master', 'PhD', 'Master', 'Bachelor']\n",
    "}\n",
    "\n",
    "df_categorical = pd.DataFrame(data_categorical)\n",
    "print(\"\\nCategorical Data:\")\n",
    "print(df_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c9adae-acc3-4910-875a-f1762921e75d",
   "metadata": {},
   "source": [
    "Q21.What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "287854d1-3736-4680-a4c3-721427265182",
   "metadata": {},
   "source": [
    "What is Feature Scaling?\n",
    "Feature scaling refers to the process of transforming the features (input variables) of a dataset to a common scale, without distorting the differences in the ranges of values. The goal of feature scaling is to make the learning algorithm perform better by giving equal importance to all features, regardless of their original units or ranges.\n",
    "\n",
    "In many machine learning algorithms, the model’s performance can improve significantly when features are scaled. Algorithms that use distance-based calculations or gradient-based optimization often require feature scaling to work effectively.\n",
    "\n",
    "Why is Feature Scaling Important?\n",
    "Some machine learning algorithms are sensitive to the magnitude of the features and can perform poorly if the features have very different scales. Here are the key reasons why feature scaling is important:\n",
    "\n",
    "Improving Model Convergence (For Gradient Descent-based Algorithms):\n",
    "\n",
    "In optimization algorithms like Gradient Descent, feature scaling can speed up convergence by making the gradient steps more uniform. If one feature has a large range and another has a small range, the optimization process may take longer or get stuck in inefficient local minima.\n",
    "Ensuring Equal Weighting of Features:\n",
    "\n",
    "If features have different scales, the model may incorrectly assign more importance to features with larger numerical ranges, leading to biased results. Feature scaling ensures that all features contribute equally to the model.\n",
    "Distance-Based Algorithms:\n",
    "\n",
    "Algorithms like K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and K-Means Clustering rely on calculating distances (e.g., Euclidean distance) between data points. If one feature has a much larger range than others, it can dominate the distance calculation, leading to inaccurate predictions.\n",
    "Improving Regularization:\n",
    "\n",
    "In models that use regularization (e.g., Ridge Regression, Lasso Regression), feature scaling ensures that the regularization term applies uniformly across features, preventing over-penalization of smaller features.\n",
    "How Does Feature Scaling Help in Machine Learning?\n",
    "Feature scaling ensures that the model treats each feature on equal footing, regardless of its original unit of measurement. This can lead to:\n",
    "\n",
    "Better Model Performance: The model can learn more effectively and converge faster if all features are on a similar scale.\n",
    "Improved Accuracy: Especially for distance-based and gradient descent-based models, feature scaling can result in better model accuracy.\n",
    "Faster Training: Scaling speeds up the training process, as models will converge more quickly, avoiding the need for excessively many iterations due to unbalanced feature scales\n",
    "\n",
    "When to Use Feature Scaling?\n",
    "Distance-based algorithms: Feature scaling is crucial for algorithms like K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and K-Means Clustering, where distance between points matters.\n",
    "Gradient descent-based algorithms: Algorithms like Linear Regression, Logistic Regression, and Neural Networks will benefit from feature scaling, as it helps in faster convergence of the optimization process.\n",
    "Principal Component Analysis (PCA): When using dimensionality reduction techniques like PCA, feature scaling is important because PCA maximizes variance, and features with larger scales can dominate the principal components.\n",
    "When Not to Use Feature Scaling?\n",
    "Tree-based models: Algorithms like Decision Trees, Random Forests, and Gradient Boosting are not affected by feature scaling. This is because these models make decisions based on splits of the data, which are not influenced by the magnitude of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e8d2b-6a17-4dec-9291-093ac170b543",
   "metadata": {},
   "source": [
    "Q22.How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95409b8d-ecd9-45f4-9d6f-b5fa7ef74929",
   "metadata": {},
   "source": [
    "In Python, scaling or feature normalization is typically performed using the scikit-learn library, which provides several classes and functions for scaling and preprocessing data. The most commonly used scalers are:\n",
    "\n",
    "Min-Max Scaling (Normalization) - MinMaxScaler\n",
    "Standardization (Z-score Normalization) - StandardScaler\n",
    "Robust Scaling - RobustScaler\n",
    "MaxAbs Scaling - MaxAbsScaler\n",
    "\n",
    "1. Min-Max Scaling (Normalization)\n",
    "The MinMaxScaler scales the data to a specific range, often between 0 and 1.\n",
    "\n",
    "Steps:\n",
    "Initialize the MinMaxScaler.\n",
    "Fit and transform the data.\n",
    "\n",
    "2. Standardization (Z-score Normalization)\n",
    "StandardScaler standardizes the data by removing the mean and scaling it to unit variance. This results in a distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Steps:\n",
    "Initialize the StandardScaler.\n",
    "Fit and transform the data.\n",
    "\n",
    "3. Robust Scaling\n",
    "RobustScaler scales the data using the median and the interquartile range (IQR). This makes it robust to outliers since it is based on percentiles rather than the mean and standard deviation.\n",
    "\n",
    "Steps:\n",
    "Initialize the RobustScaler.\n",
    "Fit and transform the data.\n",
    "\n",
    "4. MaxAbs Scaling\n",
    "MaxAbsScaler scales the data to the range [-1, 1] based on the maximum absolute value of each feature. This scaler does not shift the data (no mean subtraction).\n",
    "\n",
    "Steps:\n",
    "Initialize the MaxAbsScaler.\n",
    "Fit and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2ac3992-8566-4036-9b35-e7c248f20b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[1 5]\n",
      " [2 6]\n",
      " [3 7]\n",
      " [4 8]\n",
      " [5 9]]\n",
      "\n",
      "Scaled Data (Standardization):\n",
      "[[-1.41421356 -1.41421356]\n",
      " [-0.70710678 -0.70710678]\n",
      " [ 0.          0.        ]\n",
      " [ 0.70710678  0.70710678]\n",
      " [ 1.41421356  1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "X = np.array([[1, 5], [2, 6], [3, 7], [4, 8], [5, 9]])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(X)\n",
    "print(\"\\nScaled Data (Standardization):\")\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69270e54-537b-437e-96e1-525437ccc48f",
   "metadata": {},
   "source": [
    "Q23.What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40788745-c3d3-42d6-a40a-0ad508bf473d",
   "metadata": {},
   "source": [
    "What is sklearn.preprocessing?\n",
    "sklearn.preprocessing is a module in the scikit-learn library that provides a collection of tools for data preprocessing. The purpose of this module is to transform and prepare the input data (features) so that it is suitable for machine learning models. It includes various functions for scaling, encoding, and imputing missing values, among other things.\n",
    "\n",
    "Data preprocessing is an essential step in the machine learning pipeline because raw data often comes in various forms and formats that may not be suitable for modeling. The preprocessing module helps in transforming the raw data into a clean, standardized format for better performance of machine learning algorithms.\n",
    "\n",
    "Key Functions and Classes in sklearn.preprocessing\n",
    "Here are the main functionalities provided by the sklearn.preprocessing module:\n",
    "\n",
    "1. Scaling and Normalization\n",
    "Scaling and normalization techniques adjust the range and distribution of features so that they are more comparable across all features. This is particularly important for distance-based models and models that use gradient descent optimization.\n",
    "\n",
    "MinMaxScaler: Scales data to a specified range, usually [0, 1].\n",
    "StandardScaler: Standardizes features by removing the mean and scaling them to unit variance (z-score normalization).\n",
    "RobustScaler: Scales features using the median and interquartile range (IQR), which is robust to outliers.\n",
    "MaxAbsScaler: Scales data by dividing by the maximum absolute value for each feature.\n",
    "Normalizer: Scales individual samples (rows) to have unit norm.\n",
    "\n",
    "2. Encoding Categorical Variables\n",
    "Machine learning algorithms generally require numerical input, but many datasets include categorical features (e.g., gender, product_type, country). sklearn.preprocessing provides several methods to encode categorical data.\n",
    "\n",
    "LabelEncoder: Encodes labels (categories) into integers.\n",
    "OneHotEncoder: Converts categorical features into one-hot encoded vectors (binary columns).\n",
    "OrdinalEncoder: Encodes ordinal categorical features, where there is a meaningful order between categories.\n",
    "\n",
    "3. Imputing Missing Values\n",
    "Many datasets have missing values. The sklearn.preprocessing module provides tools to handle missing data by filling in the gaps.\n",
    "\n",
    "Imputer (SimpleImputer): This is the most commonly used tool to fill missing values. It can replace missing values with mean, median, most frequent value, or a constant.\n",
    "\n",
    "4. Binarization\n",
    "Sometimes, we need to convert numeric features into binary values (0 or 1). This is typically done by applying a threshold to the data.\n",
    "\n",
    "Binarizer: This tool transforms features into binary values based on a threshold. Values greater than the threshold are set to 1, and those less than or equal to the threshold are set to 0.\n",
    "\n",
    "5. Polynomial Features\n",
    "For some machine learning models, creating polynomial features (combinations of features) can improve performance. The PolynomialFeatures class generates interaction features and polynomial features of the original dataset.\n",
    "\n",
    "PolynomialFeatures: Generates polynomial and interaction features.\n",
    "\n",
    "When to Use sklearn.preprocessing?\n",
    "Feature Scaling: Use when your machine learning algorithm depends on the scale of features (e.g., K-Nearest Neighbors, Support Vector Machines, Linear Regression).\n",
    "Categorical Data: Use when you have categorical features that need to be converted into numerical format (e.g., Label Encoding, One-Hot Encoding).\n",
    "Handling Missing Data: Use when your dataset has missing values (SimpleImputer).\n",
    "Creating Polynomial Features: Use when you want to add polynomial interactions to your features (for polynomial regression or nonlinear models)\n",
    "\n",
    "Example: A Pipeline with sklearn.preprocessing\n",
    "In practice, data preprocessing is often part of a larger pipeline where different preprocessing steps are chained together. For example, scaling and encoding might be performed sequentially before fitting a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7221541b-1453-4df4-97d9-0bc9344fb132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = [['cat', 10], ['dog', 20], ['cat', 30], ['dog', 40]]\n",
    "y = [0, 1, 0, 1]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), [0]), \n",
    "        ('num', StandardScaler(), [1])\n",
    "    ]\n",
    ")\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "pipeline.fit(X, y)\n",
    "predictions = pipeline.predict(X)\n",
    "\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36202b62-ad1f-4022-bfde-da4ba2f4a0b0",
   "metadata": {},
   "source": [
    "Q24.How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41d34163-82ec-4efd-bad5-01181b8e8f58",
   "metadata": {},
   "source": [
    "To split your data into training and testing sets in Python, you typically use train_test_split from the sklearn.model_selection module. This function is an easy way to divide your dataset into two parts: one for training the model and one for testing its performance.\n",
    "\n",
    "How to Split Data for Model Fitting\n",
    "Training Data: The data that the model will learn from (usually 70-80% of the total dataset).\n",
    "Test Data: The data used to evaluate the model's performance (usually 20-30% of the total dataset).\n",
    "Steps to Split Data in Python Using train_test_split:\n",
    "Step 1: Import the necessary libraries.\n",
    "Step 2: Prepare your data.\n",
    "Step 3: Use train_test_split to split the data.\n",
    "Step 4: Optionally, shuffle the data before splitting (done by default in train_test_split).\n",
    "\n",
    "Syntax of train_test_split:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X: Features (independent variables).\n",
    "y: Target variable (dependent variable).\n",
    "test_size: Proportion of the dataset to include in the test split (float between 0 and 1). test_size=0.2 means 20% of the data will be used for testing.\n",
    "random_state: Ensures reproducibility. Setting it to an integer (e.g., 42) makes the split reproducible across different runs.\n",
    "train_size (optional): Proportion of the dataset to include in the train split. It can be used instead of test_size.\n",
    "shuffle (optional): Whether to shuffle the data before splitting. By default, this is set to True.\n",
    "stratify (optional): If you want to split the data in such a way that the distribution of the target variable in the train and test sets is similar (useful for imbalanced datasets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a48dcb02-17a2-425b-80bf-01c6ee8ec0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features (X_train):\n",
      "   Feature1  Feature2\n",
      "5         6        60\n",
      "2         3        30\n",
      "4         5        50\n",
      "3         4        40\n",
      "\n",
      "Testing Features (X_test):\n",
      "   Feature1  Feature2\n",
      "0         1        10\n",
      "1         2        20\n",
      "\n",
      "Training Target (y_train):\n",
      "5    1\n",
      "2    0\n",
      "4    0\n",
      "3    1\n",
      "Name: Target, dtype: int64\n",
      "\n",
      "Testing Target (y_test):\n",
      "0    0\n",
      "1    1\n",
      "Name: Target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Feature1': [1, 2, 3, 4, 5, 6],\n",
    "        'Feature2': [10, 20, 30, 40, 50, 60],\n",
    "        'Target': [0, 1, 0, 1, 0, 1]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df[['Feature1', 'Feature2']]\n",
    "y = df['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(\"Training Features (X_train):\")\n",
    "print(X_train)\n",
    "print(\"\\nTesting Features (X_test):\")\n",
    "print(X_test)\n",
    "print(\"\\nTraining Target (y_train):\")\n",
    "print(y_train)\n",
    "print(\"\\nTesting Target (y_test):\")\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed575f-f8f7-4e82-9fbb-b07d8f59578f",
   "metadata": {},
   "source": [
    "Q25.Explain data encoding?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80accc6d-38e6-4f89-bf5c-c3e0501a2887",
   "metadata": {},
   "source": [
    "What is Data Encoding?\n",
    "Data encoding refers to the process of converting categorical data (i.e., non-numeric values) into a numeric format that can be fed into machine learning algorithms. Most machine learning algorithms, such as linear regression, decision trees, and neural networks, work with numerical data, so categorical variables need to be transformed into a numeric format before they can be used.\n",
    "\n",
    "Categorical data can come in different forms, such as:\n",
    "\n",
    "Nominal data: Categories with no specific order (e.g., Color: Red, Green, Blue).\n",
    "Ordinal data: Categories with a defined order but no fixed interval between them (e.g., Size: Small, Medium, Large).\n",
    "Why Do We Need Data Encoding?\n",
    "Most machine learning models (like decision trees, linear regression, and neural networks) require numerical data because:\n",
    "\n",
    "They calculate distances (e.g., Euclidean distance), which require numeric input.\n",
    "Many models use mathematical functions and calculations (e.g., dot products in linear regression), which also require numeric input.\n",
    "Common Methods of Encoding Categorical Data\n",
    "There are several techniques for encoding categorical data into numerical format. Below are the most commonly used methods in machine learning:\n",
    "\n",
    "1. Label Encoding\n",
    "Label Encoding converts each category into a unique integer. Each unique value in the category is assigned a numerical label starting from 0.\n",
    "\n",
    "When to use:\n",
    "Label encoding is useful when there is a natural ordering (ordinal data), like Low, Medium, High for a Size feature.\n",
    "Be careful with nominal data, as label encoding can unintentionally introduce an ordinal relationship.\n",
    "\n",
    "2. One-Hot Encoding\n",
    "One-Hot Encoding is a more sophisticated method where each category is transformed into a new binary column (0 or 1). If a category exists in the sample, the corresponding column gets a 1; otherwise, it gets a 0.\n",
    "\n",
    "When to use:\n",
    "One-Hot Encoding is ideal for nominal data (where there’s no inherent order), as it avoids implying any ordinal relationship.\n",
    "\n",
    "3. Ordinal Encoding\n",
    "Ordinal Encoding is a method that assigns each category an integer value based on the natural order of the categories. This is useful when you have ordinal data, where there is a meaningful order between categories.\n",
    "\n",
    "When to use:\n",
    "Ordinal encoding is ideal when the categories have an inherent order but not necessarily equal spacing between them (e.g., Low, Medium, High).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "717b4dc9-c263-448a-a73b-873d84405f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sizes = ['Small', 'Medium', 'Large', 'Medium', 'Small']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "encoded_sizes = label_encoder.fit_transform(sizes)\n",
    "\n",
    "print(encoded_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c7fe93-39bf-4102-9a92-019dc25668f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
